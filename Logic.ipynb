{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a00ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    AdaBoostClassifier,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb9081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random_states = random.sample(range(1, 100), 50)\n",
    "random_states = [73, 62, 81, 4, 65, 10, 85, 3, 54, 9, 21, 13, 69, 52, 16, 96, 50, 8, 44, 92, 30, 56, 89, 86, 39, 70, 90, 17, 75, 49, 43, 45, 58, 22, 15, 80, 97, 66, 68, 55, 34, 63, 42, 12, 6, 29, 57, 25, 36, 41]\n",
    "print(random_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbe5eca",
   "metadata": {},
   "source": [
    "# Loading and Preprocessing The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28eff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"PATH TO DATA\")\n",
    "\n",
    "label_map = {'Case': 0, 'Control': 1}\n",
    "data[\"Label\"] = data[\"Label\"].map(label_map)\n",
    "\n",
    "x_values = data.iloc[:, 1:].values\n",
    "y_values = data.iloc[:, 0].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_values = scaler.fit_transform(x_values)\n",
    "\n",
    "mi_scores = mutual_info_regression(x_values, y_values,random_state= 42)\n",
    "\n",
    "feature_names = data.columns[1:]\n",
    "\n",
    "sorted_features = [f for _, f in sorted(zip(mi_scores, feature_names), reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c29d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted_features)\n",
    "print(\"====================\")\n",
    "print(len(sorted_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca243131",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_mi_scores = sorted(mi_scores, reverse=True)\n",
    "valid_score_count = len([score for score in mi_scores if score > 0])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(sorted_features, sorted_mi_scores)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Mutual Information Scores')\n",
    "plt.title('Mutual Information Scores for Features (Sorted)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b264cd0",
   "metadata": {},
   "source": [
    "# Plotting FI, Pairplot and CM for all Related features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192f7c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "All_Related_Features = sorted_features[:valid_score_count]\n",
    "\n",
    "print(\"Related features:\", All_Related_Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1082b0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = All_Related_Features\n",
    "data = data[['Label'] + important_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7129f8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pair plot\n",
    "sns.pairplot(data, hue='Label', diag_kind='kde', markers=['o', 's'], palette='husl')\n",
    "plt.title('Pair Plot of Data')\n",
    "plt.show()\n",
    "\n",
    "# Create a correlation matrix\n",
    "corr_matrix = data.corr()\n",
    "\n",
    "# Plot heatmap of the correlation matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72bdef5",
   "metadata": {},
   "source": [
    "# Model Training and Testing using All related variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f351b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = data.iloc[:, 1:].values\n",
    "y_values = data.iloc[:, 0].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_values = scaler.fit_transform(x_values)\n",
    "\n",
    "f1_scores = []\n",
    "\n",
    "for num_features in range(1, len(All_Related_Features) + 1):\n",
    "    selected_features = All_Related_Features[:num_features]\n",
    "\n",
    "    x_subset = data[selected_features].values\n",
    "\n",
    "    average_f1 = 0\n",
    "\n",
    "    for random_state in random_states:\n",
    "        base_models = [\n",
    "            RandomForestClassifier(n_estimators=100, random_state=random_state),\n",
    "            GradientBoostingClassifier(n_estimators=100, random_state=random_state),\n",
    "            AdaBoostClassifier(n_estimators=100, random_state=random_state),\n",
    "            DecisionTreeClassifier(random_state=random_state),\n",
    "            LogisticRegression(max_iter=10000),\n",
    "            SVC(probability=True),\n",
    "            GaussianNB(),\n",
    "            KNeighborsClassifier(),\n",
    "            CatBoostClassifier(iterations=100, random_seed=random_state, verbose=False),\n",
    "            XGBClassifier(random_state=random_state),\n",
    "            LGBMClassifier(random_state=random_state)\n",
    "    ]\n",
    "\n",
    "        meta_model = LogisticRegression(max_iter=10000)\n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_subset, y_values, test_size=0.3, random_state=random_state)\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        for base_model in base_models:\n",
    "            base_model.fit(x_train, y_train)\n",
    "            y_pred_base = base_model.predict(x_test)\n",
    "            predictions.append(y_pred_base)\n",
    "\n",
    "        stacked_predictions = np.column_stack(predictions)\n",
    "\n",
    "        meta_model.fit(stacked_predictions, y_test)\n",
    "\n",
    "        y_pred_stacked = meta_model.predict(stacked_predictions)\n",
    "\n",
    "        f1 = f1_score(y_test, y_pred_stacked)\n",
    "\n",
    "        average_f1 += f1\n",
    "\n",
    "    average_f1 /= len(random_states)\n",
    "    f1_scores.append(average_f1)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(All_Related_Features) + 1), f1_scores, marker='o')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Average F1 Score')\n",
    "plt.title('F1 Score vs. Number of Features')\n",
    "plt.xticks(range(1, len(All_Related_Features) + 1))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b6488b",
   "metadata": {},
   "source": [
    "# Model Training and Testing using All related variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed74ebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = data.iloc[:, 1:].values\n",
    "y_values = data.iloc[:, 0].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_values = scaler.fit_transform(x_values)\n",
    "\n",
    "top_results = {}\n",
    "average_accuracy = 0\n",
    "average_precision = 0\n",
    "average_recall = 0\n",
    "average_f1 = 0\n",
    "average_specificity = 0\n",
    "\n",
    "for random_state in random_states:\n",
    "    base_models = [\n",
    "        RandomForestClassifier(n_estimators=100, random_state=random_state),\n",
    "        GradientBoostingClassifier(n_estimators=100, random_state=random_state),\n",
    "        AdaBoostClassifier(n_estimators=100, random_state=random_state),\n",
    "        DecisionTreeClassifier(random_state=random_state),\n",
    "        LogisticRegression(),\n",
    "        SVC(probability=True),\n",
    "        GaussianNB(),\n",
    "        KNeighborsClassifier(),\n",
    "        CatBoostClassifier(iterations=100, random_seed=random_state, verbose=False),\n",
    "        XGBClassifier(random_state=random_state),\n",
    "        LGBMClassifier(random_state=random_state)\n",
    "    ]\n",
    "\n",
    "    meta_model = LogisticRegression()\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_values, y_values, test_size=0.3, random_state=random_state)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for base_model in base_models:\n",
    "        base_model.fit(x_train, y_train)\n",
    "        y_pred_base = base_model.predict(x_test)\n",
    "        predictions.append(y_pred_base)\n",
    "\n",
    "    stacked_predictions = np.column_stack(predictions)\n",
    "\n",
    "    meta_model.fit(stacked_predictions, y_test)\n",
    "\n",
    "    y_pred_stacked = meta_model.predict(stacked_predictions)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred_stacked)\n",
    "    average_accuracy += accuracy\n",
    "\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred_stacked)\n",
    "    tn, fp, fn, tp = confusion_mat.ravel()\n",
    "\n",
    "    specificity = tn / (tn + fp)\n",
    "    recall = recall_score(y_test, y_pred_stacked)\n",
    "    precision = precision_score(y_test, y_pred_stacked)\n",
    "    f1 = f1_score(y_test, y_pred_stacked)\n",
    "\n",
    "    average_recall += recall\n",
    "    average_precision += precision\n",
    "    average_f1 += f1\n",
    "    average_specificity += specificity\n",
    "\n",
    "    top_results[random_state] = {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1, 'specificity': specificity}\n",
    "\n",
    "average_accuracy /= len(random_states)\n",
    "average_precision /= len(random_states)\n",
    "average_recall /= len(random_states)\n",
    "average_f1 /= len(random_states)\n",
    "average_specificity /= len(random_states)\n",
    "\n",
    "print(\"Average Accuracy:\", average_accuracy)\n",
    "print(\"Average Precision:\", average_precision)\n",
    "print(\"Average Recall (Sn):\", average_recall)\n",
    "print(\"Average Specificity (Sp):\", average_specificity)\n",
    "print(\"Average F1-score:\", average_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eebb750",
   "metadata": {},
   "source": [
    "# Each model using all related variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2b3804",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = data.iloc[:, 1:].values\n",
    "y_values = data.iloc[:, 0].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_values = scaler.fit_transform(x_values)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0.0\n",
    "    \n",
    "    return accuracy, precision, recall, f1, specificity\n",
    "\n",
    "base_models = [\n",
    "    RandomForestClassifier(n_estimators=100, random_state=random_states[0]),\n",
    "    GradientBoostingClassifier(n_estimators=100, random_state=random_states[0]),\n",
    "    AdaBoostClassifier(n_estimators=100, random_state=random_states[0]),\n",
    "    DecisionTreeClassifier(random_state=random_states[0]),\n",
    "    LogisticRegression(max_iter=10000),\n",
    "    SVC(probability=True),\n",
    "    GaussianNB(),\n",
    "    KNeighborsClassifier(),\n",
    "    CatBoostClassifier(iterations=100, random_seed=random_states[0], verbose=False),\n",
    "    XGBClassifier(random_state=random_states[0]),\n",
    "    LGBMClassifier(random_state=random_states[0])\n",
    "]\n",
    "\n",
    "for base_model in base_models:\n",
    "    print(f\"Results for {base_model.__class__.__name__}:\\n\")\n",
    "\n",
    "    average_metrics = {\n",
    "        'accuracy': 0,\n",
    "        'precision': 0,\n",
    "        'recall': 0,\n",
    "        'f1': 0,\n",
    "        'specificity': 0\n",
    "    }\n",
    "\n",
    "    x_subset = x_values\n",
    "\n",
    "    metrics_sum = {key: 0 for key in average_metrics}\n",
    "\n",
    "    for random_state in random_states:\n",
    "        meta_model = LogisticRegression(max_iter=10000)\n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_subset, y_values, test_size=0.3, random_state=random_state)\n",
    "\n",
    "        base_model.fit(x_train, y_train)\n",
    "        y_pred_base = base_model.predict(x_test)\n",
    "\n",
    "        stacked_predictions = y_pred_base.reshape(-1, 1)  # Reshape to column vector\n",
    "\n",
    "        meta_model.fit(stacked_predictions, y_test)\n",
    "\n",
    "        y_pred_stacked = meta_model.predict(stacked_predictions)\n",
    "\n",
    "        metrics = calculate_metrics(y_test, y_pred_stacked)\n",
    "\n",
    "        for i, key in enumerate(average_metrics):\n",
    "            metrics_sum[key] += metrics[i]\n",
    "\n",
    "    for key in average_metrics:\n",
    "        average_metrics[key] = metrics_sum[key] / len(random_states)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Average Accuracy: {average_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Average Precision: {average_metrics['precision']:.4f}\")\n",
    "    print(f\"Average Recall: {average_metrics['recall']:.4f}\")\n",
    "    print(f\"Average F1 Score: {average_metrics['f1']:.4f}\")\n",
    "    print(f\"Average Specificity: {average_metrics['specificity']:.4f}\\n\")\n",
    "\n",
    "    print(\"=\"*40 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f254ca92",
   "metadata": {},
   "source": [
    "# Model Results using Top 26 selected variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e83318",
   "metadata": {},
   "outputs": [],
   "source": [
    "Top_Selected= sorted_features[:26]\n",
    "\n",
    "print(\"Related features:\", Top_Selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34bce17",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data[['Label'] + Top_Selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e0e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = data2.iloc[:, 1:].values\n",
    "y_values = data2.iloc[:, 0].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_values = scaler.fit_transform(x_values)\n",
    "\n",
    "top_results = {}\n",
    "average_accuracy = 0\n",
    "average_precision = 0\n",
    "average_recall = 0\n",
    "average_f1 = 0\n",
    "average_specificity = 0\n",
    "average_auc = 0\n",
    "\n",
    "for random_state in random_states:\n",
    "    base_models = [\n",
    "        RandomForestClassifier(n_estimators=100, random_state=random_state),\n",
    "        GradientBoostingClassifier(n_estimators=100, random_state=random_state),\n",
    "        AdaBoostClassifier(n_estimators=100, random_state=random_state),\n",
    "        DecisionTreeClassifier(random_state=random_state),\n",
    "        LogisticRegression(),\n",
    "        SVC(probability=True),\n",
    "        GaussianNB(),\n",
    "        KNeighborsClassifier(),\n",
    "        CatBoostClassifier(iterations=100, random_seed=random_state, verbose=False),\n",
    "        XGBClassifier(random_state=random_state),\n",
    "        LGBMClassifier(random_state=random_state)\n",
    "    ]\n",
    "\n",
    "    meta_model = LogisticRegression()\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_values, y_values, test_size=0.3, random_state=random_state)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for base_model in base_models:\n",
    "        base_model.fit(x_train, y_train)\n",
    "        y_pred_base = base_model.predict(x_test)\n",
    "        predictions.append(y_pred_base)\n",
    "\n",
    "    stacked_predictions = np.column_stack(predictions)\n",
    "\n",
    "    meta_model.fit(stacked_predictions, y_test)\n",
    "\n",
    "    y_pred_stacked = meta_model.predict(stacked_predictions)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred_stacked)\n",
    "    average_accuracy += accuracy\n",
    "\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred_stacked)\n",
    "    tn, fp, fn, tp = confusion_mat.ravel()\n",
    "\n",
    "    specificity = tn / (tn + fp)\n",
    "    recall = recall_score(y_test, y_pred_stacked)\n",
    "    precision = precision_score(y_test, y_pred_stacked)\n",
    "    f1 = f1_score(y_test, y_pred_stacked)\n",
    "\n",
    "    average_recall += recall\n",
    "    average_precision += precision\n",
    "    average_f1 += f1\n",
    "    average_specificity += specificity\n",
    "    \n",
    "     # AUC calculation\n",
    "    stacked_probabilities = meta_model.predict_proba(stacked_predictions)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, stacked_probabilities)\n",
    "    auc = roc_auc_score(y_test, stacked_probabilities)\n",
    "    top_results[random_state] = {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1, 'specificity': specificity, 'auc': auc}\n",
    "    average_auc += auc\n",
    "\n",
    "average_accuracy /= len(random_states)\n",
    "average_precision /= len(random_states)\n",
    "average_recall /= len(random_states)\n",
    "average_f1 /= len(random_states)\n",
    "average_specificity /= len(random_states)\n",
    "average_auc/= len(random_states)\n",
    "\n",
    "\n",
    "print(\"Average Accuracy:\", average_accuracy)\n",
    "print(\"Average Precision:\", average_precision)\n",
    "print(\"Average Recall (Sn):\", average_recall)\n",
    "print(\"Average Specificity (Sp):\", average_specificity)\n",
    "print(\"Average F1-score:\", average_f1)\n",
    "print(\"Average AUC:\", average_auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6d7359",
   "metadata": {},
   "source": [
    "# Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a1f507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a correlation matrix\n",
    "corr_matrix = data2.corr()\n",
    "\n",
    "# Plot heatmap of the correlation matrix without numbers\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap for the Top 26 vaiable')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40588af0",
   "metadata": {},
   "source": [
    "# Training and testing each model seperatly using 26 variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5381ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = data2.iloc[:, 1:].values\n",
    "y_values = data2.iloc[:, 0].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_values = scaler.fit_transform(x_values)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0.0\n",
    "    \n",
    "    return accuracy, precision, recall, f1, specificity\n",
    "\n",
    "base_models = [\n",
    "    RandomForestClassifier(n_estimators=100, random_state=random_states[0]),\n",
    "    GradientBoostingClassifier(n_estimators=100, random_state=random_states[0]),\n",
    "    AdaBoostClassifier(n_estimators=100, random_state=random_states[0]),\n",
    "    DecisionTreeClassifier(random_state=random_states[0]),\n",
    "    LogisticRegression(max_iter=10000),\n",
    "    SVC(probability=True),\n",
    "    GaussianNB(),\n",
    "    KNeighborsClassifier(),\n",
    "    CatBoostClassifier(iterations=100, random_seed=random_states[0], verbose=False),\n",
    "    XGBClassifier(random_state=random_states[0]),\n",
    "    LGBMClassifier(random_state=random_states[0])\n",
    "]\n",
    "\n",
    "for base_model in base_models:\n",
    "    print(f\"Results for {base_model.__class__.__name__}:\\n\")\n",
    "\n",
    "    average_metrics = {\n",
    "        'accuracy': 0,\n",
    "        'precision': 0,\n",
    "        'recall': 0,\n",
    "        'f1': 0,\n",
    "        'specificity': 0\n",
    "    }\n",
    "\n",
    "    x_subset = x_values\n",
    "\n",
    "    metrics_sum = {key: 0 for key in average_metrics}\n",
    "\n",
    "    for random_state in random_states:\n",
    "        meta_model = LogisticRegression(max_iter=10000)\n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_subset, y_values, test_size=0.3, random_state=random_state)\n",
    "\n",
    "        base_model.fit(x_train, y_train)\n",
    "        y_pred_base = base_model.predict(x_test)\n",
    "\n",
    "        stacked_predictions = y_pred_base.reshape(-1, 1)  # Reshape to column vector\n",
    "\n",
    "        meta_model.fit(stacked_predictions, y_test)\n",
    "\n",
    "        y_pred_stacked = meta_model.predict(stacked_predictions)\n",
    "\n",
    "        metrics = calculate_metrics(y_test, y_pred_stacked)\n",
    "\n",
    "        for i, key in enumerate(average_metrics):\n",
    "            metrics_sum[key] += metrics[i]\n",
    "\n",
    "    for key in average_metrics:\n",
    "        average_metrics[key] = metrics_sum[key] / len(random_states)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Average Accuracy: {average_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Average Precision: {average_metrics['precision']:.4f}\")\n",
    "    print(f\"Average Recall: {average_metrics['recall']:.4f}\")\n",
    "    print(f\"Average F1 Score: {average_metrics['f1']:.4f}\")\n",
    "    print(f\"Average Specificity: {average_metrics['specificity']:.4f}\\n\")\n",
    "\n",
    "    print(\"=\"*40 + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
